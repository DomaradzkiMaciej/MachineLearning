{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeRy_Ng0lfDT"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?id=1_utx_ZGclmCwNttSe40kYA6VHzNocdET' height=\"60\"></center>\n",
    "\n",
    "AI TECH - Akademia Innowacyjnych Zastosowań Technologii Cyfrowych. Program Operacyjny Polska Cyfrowa na lata 2014-2020\n",
    "<hr>\n",
    "\n",
    "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>\n",
    "\n",
    "<center>\n",
    "Projekt współfinansowany ze środków Unii Europejskiej w ramach Europejskiego Funduszu Rozwoju Regionalnego \n",
    "Program Operacyjny Polska Cyfrowa na lata 2014-2020,\n",
    "Oś Priorytetowa nr 3 \"Cyfrowe kompetencje społeczeństwa\" Działanie  nr 3.2 \"Innowacyjne rozwiązania na rzecz aktywizacji cyfrowej\" \n",
    "Tytuł projektu:  „Akademia Innowacyjnych Zastosowań Technologii Cyfrowych (AI Tech)”\n",
    "    </center>\n",
    "\n",
    "**Author: Jacek Sroka**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uEERM15Z3i6"
   },
   "source": [
    "# ML in big scale - LAB 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao9tweXeZr_i"
   },
   "source": [
    "Plan\n",
    "\n",
    "1. Installation\n",
    "2. Introduction to RDD API\n",
    "3. Broadcast variables\n",
    "4. Homework\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5XTROAjE7ek"
   },
   "source": [
    "To **edit the colab** first copy it to your drive with the top bars `File -> Save a copy on drive` option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdrOqKrbbWEi"
   },
   "source": [
    "## Installation\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxqy7fQ2Zn_u",
    "outputId": "66634c44-9f43-4604-89aa-052efbfaa415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 281.3 MB 50 kB/s \n",
      "\u001b[K     |████████████████████████████████| 199 kB 41.1 MB/s \n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark --quiet\n",
    "!pip install -U -q PyDrive --quiet \n",
    "!apt install openjdk-8-jdk-headless &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvP2cvhBb-gi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRX13sVIcU9Z"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"mlibs\").setMaster(\"local[*]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC4mnbWTYtkH"
   },
   "source": [
    "Note: pyspark documentation is [here](https://spark.apache.org/docs/3.1.2/api/python/reference/index.html). For example: list of `SparkContext` methods is [here](https://spark.apache.org/docs/3.1.2/api/python/reference/pyspark.html#spark-context-apis).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUtLR5z2dUNv"
   },
   "source": [
    "## Introduction to RDD API\n",
    "We start with a sample RDD obtained from a local Python collection. The second parameter of `parallelize` specifies how many partitions we want to have and thus allows us to control the parallelism. Usually it is good to have 4x more partitions than cores, but having partitions with too few elements can make the overhead noticable. On the other hand for some algorithms the rate of data transfer over the network can grow quickly with the number of partitions (for example the algorithm could operate on partition pairs).\n",
    "\n",
    "`cache` suggests to Spark to keep this RDD in cluster memory to speed up future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_NdrZeQNO5v"
   },
   "outputs": [],
   "source": [
    "input_rdd = sc.parallelize(range(1, 10), 2).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zm-geH4uYNLZ"
   },
   "source": [
    "`collect` converts the rdd into local python collection on the driver side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFYEZ4rCYDLx",
    "outputId": "2a2d0624-15b9-4707-e9da-c4fba4763f60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSqNOasqW7ew"
   },
   "source": [
    "Try actions: `first`, `count`, `take(n)` and `takeSample(withReplacement, num, [seed])`. The documentation can be found [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ektM1eJWg3A6",
    "outputId": "edee7dcf-2b62-46cc-fde7-778ce5ba428c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 9, [1, 2, 3, 4, 5], [4, 8, 3, 9, 4]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[input_rdd.first(),\n",
    " input_rdd.count(),\n",
    " input_rdd.take(5),\n",
    " input_rdd.takeSample(True, 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwIK_z-B8YVy"
   },
   "source": [
    "Compute the total sum of the RDD elements with `reduce(func)`.\n",
    "\n",
    "RDD API with all the actions and transformations can be found [here](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html#pyspark.RDD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEgBWIsk87vh",
    "outputId": "287e6dd2-fe3f-49f0-f4db-4ca2e596bf46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "input_rdd.reduce(add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1WUeAj-XS43"
   },
   "source": [
    "RDD are divided into partitions and distributed on the cluster. We can check how RDD are obtained (lineage), how many partitions there are, and how much memory do they use with the `toDebugString` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "YICgWJTUWw3w",
    "outputId": "3eede878-56d8-4346-be0f-94193388f641"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'(2) PythonRDD[1] at RDD at PythonRDD.scala:53 [Memory Serialized 1x Replicated]\\n |       CachedPartitions: 2; MemorySize: 176.0 B; DiskSize: 0.0 B\\n |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274 [Memory Serialized 1x Replicated]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_rdd.toDebugString().decode('ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt9LSHutbA8N"
   },
   "source": [
    "Let us create new RDD by transforming each value of `input_rdd` with the `map` transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPVic4F6W0Yo",
    "outputId": "a3fb6be9-68b7-4bd7-a915-43679f87dea7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus1 = input_rdd.map(lambda x: x+1)\n",
    "plus1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "EUIhIxksbrZW",
    "outputId": "9b0dd0d8-db96-4c07-f8aa-a431c2bf30f7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'(2) PythonRDD[9] at collect at <ipython-input-9-8bfa8a4c60ee>:2 []\\n |  PythonRDD[1] at RDD at PythonRDD.scala:53 []\\n |      CachedPartitions: 2; MemorySize: 176.0 B; DiskSize: 0.0 B\\n |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274 []'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus1.toDebugString().decode('ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFgUHfxz-bBu"
   },
   "source": [
    "Now try the `toDebugString` on the examples from lab1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0RObatYAnl5"
   },
   "source": [
    "Now create RDD from lines of the `sample_data/README.md` file and count the lines that contain the letter `a`. For that you can use the `filter` transformation. Don't forget to cache the RDD as we will use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5VkwFnXgb6tI",
    "outputId": "04240274-a6b0-44f9-fdf3-dab87452da12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_from_file = sc.textFile('sample_data/README.md')\n",
    "lines_from_file.filter(lambda line: 'a' in line).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XU1EFUSQbI7R"
   },
   "source": [
    "Convert the `lines_of_text` RDD to RDD of words by using `flatMap`, then use  `map` and `reduceByKey` to count number of words in the MapReduce way. Note that this is similar the one of the exercises from lab 1, but here we expect you to use RDD API rather than the MapReduce API we have provided last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uez-OluTmn7F",
    "outputId": "dd14cd8f-21b0-47f0-b470-bbde6c6d5bf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('directory', 1),\n",
       " ('datasets', 1),\n",
       " ('', 51),\n",
       " ('*', 3),\n",
       " ('`california_housing_data*.csv`', 1),\n",
       " ('is', 4),\n",
       " ('housing', 1),\n",
       " ('1990', 1),\n",
       " ('more', 1),\n",
       " ('at:', 2),\n",
       " ('https://developers.google.com/machine-learning/crash-course/california-housing-data-description',\n",
       "  1),\n",
       " ('`mnist_*.csv`', 1),\n",
       " ('of', 2),\n",
       " (\"[Anscombe's\", 1),\n",
       " ('was', 2),\n",
       " ('originally', 1),\n",
       " ('in', 2),\n",
       " ('Anscombe,', 1),\n",
       " ('F.', 1),\n",
       " (\"'Graphs\", 1),\n",
       " ('American', 1),\n",
       " ('Statistician.', 1),\n",
       " ('(1):', 1),\n",
       " ('prepared', 1),\n",
       " ('This', 1),\n",
       " ('includes', 1),\n",
       " ('a', 3),\n",
       " ('few', 1),\n",
       " ('sample', 2),\n",
       " ('to', 1),\n",
       " ('get', 1),\n",
       " ('you', 1),\n",
       " ('started.', 1),\n",
       " ('California', 1),\n",
       " ('data', 1),\n",
       " ('from', 1),\n",
       " ('the', 3),\n",
       " ('US', 1),\n",
       " ('Census;', 1),\n",
       " ('information', 1),\n",
       " ('available', 1),\n",
       " ('small', 1),\n",
       " ('[MNIST', 1),\n",
       " ('database](https://en.wikipedia.org/wiki/MNIST_database),', 1),\n",
       " ('which', 1),\n",
       " ('described', 2),\n",
       " ('http://yann.lecun.com/exdb/mnist/', 1),\n",
       " ('`anscombe.json`', 1),\n",
       " ('contains', 1),\n",
       " ('copy', 2),\n",
       " ('quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet);', 1),\n",
       " ('it', 1),\n",
       " ('J.', 1),\n",
       " ('(1973).', 1),\n",
       " ('Statistical', 1),\n",
       " (\"Analysis'.\", 1),\n",
       " ('27', 1),\n",
       " ('17-21.', 1),\n",
       " ('JSTOR', 1),\n",
       " ('2682899.', 1),\n",
       " ('and', 1),\n",
       " ('our', 1),\n",
       " ('by', 1),\n",
       " ('[vega_datasets', 1),\n",
       " ('library](https://github.com/altair-viz/vega_datasets/blob/4f67bdaad10f45e3549984e17e1b3088c731503d/vega_datasets/_data/anscombe.json).',\n",
       "  1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_from_file.flatMap(lambda line: line.split(\" \")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-b-JofOnds1"
   },
   "source": [
    "Rather than process large datasets Spark can be also used to distribute computation to many machines/cores. Use Monte Carlo method to estimate the value of Pi. For that generate $n=10^5$ random points in square $[-1;1]\\times[-1;1]$ and count the proportion of the ones inside the largest circle contained by this square. Make sure that the computation can be distributed to at least 4 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ysLuXFwOmDRk",
    "outputId": "532fc2d3-31ab-4763-9cdc-6b9e80ad8bc4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Pi is roughly 3.14082'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "slices = 4\n",
    "n = 100000 * slices\n",
    "\n",
    "xs = range(n)\n",
    "rdd = sc.parallelize(xs, slices).setName(\"'Initial rdd'\")\n",
    "sample = rdd.map(lambda i: (random.uniform(-1,1), random.uniform(-1,1))).setName(\"'Random points sample'\")\n",
    "inside_circle = sample.filter(lambda tup: tup[0]**2+tup[1]**2 <= 1).setName(\"'Random points inside circle'\")\n",
    "count = inside_circle.count()\n",
    "\n",
    "f\"Pi is roughly {4.0 * count / n}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-oMbhvHbJbD"
   },
   "source": [
    "## Broadcast variables\n",
    "Did you notice that the `random` value was available to execute the lambda in `map` even though this takes place on worker nodes while the random singleton is initialized in the driver program? Let us explore this further.\n",
    "\n",
    "First let us reinitialize our spark context and make it run more local workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orEOT2UbVqbW"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"mlibs\").setMaster(\"local[8]\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlmQmoJoWXu9"
   },
   "source": [
    "Now we will create a large local list and reference it in the lambda in the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oKQGZt4oKYdH",
    "outputId": "427b6d70-163c-4928-b6ff-63b6dcfbd1b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "===========\n",
      "[[1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]]\n",
      "Iteration 0 took 3.9231453890000125 seconds\n",
      "\n",
      "Iteration 1\n",
      "===========\n",
      "[[1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]]\n",
      "Iteration 1 took 2.6186021499999868 seconds\n",
      "\n",
      "Iteration 2\n",
      "===========\n",
      "[[1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]]\n",
      "Iteration 2 took 2.5593633850000117 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "slices = 8  # change me\n",
    "list_size = 1000000  # change me\n",
    "times_to_count_len = 50  # change me\n",
    "\n",
    "list1 = list(range(list_size))\n",
    "\n",
    "for i in range(3):\n",
    "   print(f\"Iteration {i}\")\n",
    "   print(\"===========\")\n",
    "   start_time = time.perf_counter()\n",
    "   observed_sizes = sc.parallelize(range(times_to_count_len), slices).map(lambda x: len(list1))\n",
    "   # Collect the small RDD so we can print the observed sizes locally.\n",
    "   print(observed_sizes.glom().collect())\n",
    "   print(f\"Iteration {i} took {time.perf_counter() - start_time} seconds\")\n",
    "   print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTAvn0akXZ5S"
   },
   "source": [
    "Here the driver program is the bottleneck. The large list has to be transferred from it to all worker nodes. Spark has a mechanism for that called broadcast variables. Compare the running times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rN48017ISUj1",
    "outputId": "e2225770-21c4-46b9-bf41-0fc22c994efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "===========\n",
      "[[1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]]\n",
      "Iteration 0 took 0.4164461610000103 seconds\n",
      "\n",
      "Iteration 1\n",
      "===========\n",
      "[[1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]]\n",
      "Iteration 1 took 0.4595048170000098 seconds\n",
      "\n",
      "Iteration 2\n",
      "===========\n",
      "[[1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000], [1000000, 1000000, 1000000, 1000000, 1000000, 1000000, 1000000]]\n",
      "Iteration 2 took 0.4078543219999915 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "slices = 8  # change me\n",
    "list_size = 1000000  # change me\n",
    "times_to_count_len = 50  # change me\n",
    "\n",
    "list1 = list(range(list_size))\n",
    "\n",
    "for i in range(3):\n",
    "   print(f\"Iteration {i}\")\n",
    "   print(\"===========\")\n",
    "   start_time = time.perf_counter()\n",
    "\n",
    "   # all changes are in this two lines\n",
    "   blist1 = sc.broadcast(list1)\n",
    "   observed_sizes = sc.parallelize(range(times_to_count_len), slices).map(lambda x: len(blist1.value))\n",
    "   \n",
    "   # Collect the small RDD so we can print the observed sizes locally.\n",
    "   print(observed_sizes.glom().collect())\n",
    "   print(f\"Iteration {i} took {time.perf_counter() - start_time} seconds\")\n",
    "   print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvRWsc93Z3GC"
   },
   "source": [
    "## Exercise 1 and 2\n",
    "Implement matrix multiplication with both methods from the lecture. You can use the following sample data to test your Spark solution. For combining two RDDs you can use the `union` transformation. Also use `flatMap`,  `groupByKey` and `aggregateByKey/combineByKey` and `groupByKey` and `mapValues` for solution 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AV7PGRiqaSkX",
    "outputId": "dc32bb57-24e3-4acf-fcaf-203c058d9f35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M=[((0, 0), 1), ((0, 1), 2), ((0, 2), 3), ((1, 0), 4), ((1, 1), 5), ((1, 2), 6), ((2, 0), 7), ((2, 1), 8), ((2, 2), 9)]\n",
      "N=[((0, 0), 1), ((0, 1), 2), ((1, 0), 3), ((1, 1), 4), ((2, 0), 5), ((2, 1), 6)]\n",
      "MN=[((0, 0), 22), ((0, 1), 28), ((1, 0), 49), ((1, 1), 64), ((2, 0), 76), ((2, 1), 100)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def to_sparse(arr):\n",
    "  for r in range(len(arr)):\n",
    "    for c in range(len(arr[0])):\n",
    "      yield ((r,c), arr[r][c])\n",
    "\n",
    "M = [[1,2,3], [4,5,6], [7,8,9]]\n",
    "N = [[1,2], [3,4], [5,6]]\n",
    "MN = np.matmul(M,N)  # [[22,28], [49,64], [76,100]]\n",
    "\n",
    "print(f\"M={list(to_sparse(M))}\")\n",
    "print(f\"N={list(to_sparse(N))}\")\n",
    "print(f\"MN={list(to_sparse(MN))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ijNMasM3-A24",
    "outputId": "ee928da5-0024-45ba-a20d-8714bcaa584c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1), 64),\n",
       " ((2, 0), 76),\n",
       " ((0, 1), 28),\n",
       " ((0, 0), 22),\n",
       " ((1, 0), 49),\n",
       " ((2, 1), 100)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from operator import add\n",
    "\n",
    "\n",
    "I = len(M)  \n",
    "J = len(M[0])  # =len(N)\n",
    "K = len(N[0])\n",
    "\n",
    "Mrdd = sc.parallelize(to_sparse(M)).map(lambda x: (x[0][1], (x[0][0], x[1], 'M')))\n",
    "Nrdd = sc.parallelize(to_sparse(N)).map(lambda x: (x[0][0], (x[0][1], x[1], 'N')))\n",
    "\n",
    "Mrdd.join(Nrdd)\\\n",
    "    .map(lambda x: ((x[1][0][0], x[1][1][0]), x[1][0][1] * x[1][1][1]))\\\n",
    "    .reduceByKey(add)\\\n",
    "    .collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcNyNeE6ikLO",
    "outputId": "d7568fd2-2bb1-4786-ede1-cb2bb5f00f9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1), 64),\n",
       " ((2, 0), 76),\n",
       " ((0, 1), 28),\n",
       " ((0, 0), 22),\n",
       " ((1, 0), 49),\n",
       " ((2, 1), 100)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from operator import add\n",
    "\n",
    "\n",
    "I = len(M)  \n",
    "J = len(M[0])  # =len(N)\n",
    "K = len(N[0])\n",
    "\n",
    "Mrdd = sc.parallelize(to_sparse(M)).flatMap(lambda x: (((x[0][0], k), (x[0][1], x[1])) for k in range(0, K)))\n",
    "Nrdd = sc.parallelize(to_sparse(N)).flatMap(lambda x: (((i, x[0][1]), (x[0][0], x[1])) for i in range(0, I)))\n",
    "\n",
    "Mrdd.join(Nrdd)\\\n",
    "    .filter(lambda x: x[1][0][0] == x[1][1][0])\\\n",
    "    .map(lambda x: (x[0], x[1][0][1] * x[1][1][1]))\\\n",
    "    .reduceByKey(add)\\\n",
    "    .collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0klBlOWcai_8"
   },
   "source": [
    "## Exercise 3\n",
    "Your goal is to join two collections by key. One collection (A) is \"big data\" size and has to be processed on many machines and the other one (B) is small enough to fit in memory of a single machine. Implement the join by using RDD with the first collection and a broadcast variable with the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OuuIkdY9nTpO",
    "outputId": "b7259303-105b-4a8e-8d1d-85bdddeeb059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=ParallelCollectionRDD[63] at readRDDFromFile at PythonRDD.scala:274\n",
      "B=[(2, 4), (3, 4), (3, 5)]\n",
      "AjoinB=[(1, 2, 4), (1, 3, 4), (1, 3, 5), (2, 2, 4)]\n"
     ]
    }
   ],
   "source": [
    "A = [(1,2), (1,3), (2,2)]  # A(X,Y)\n",
    "B = [(2,4), (3,4), (3,5)]  # B(Y,Z)\n",
    "AjoinB = [(1,2,4), (1,3,4), (1,3,5), (2,2,4)]  # A(X,Y) join B(Y,Z) on Y\n",
    "A = sc.parallelize(A)\n",
    "\n",
    "print(f\"A={A}\")\n",
    "print(f\"B={B}\")\n",
    "print(f\"AjoinB={AjoinB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYfn7HRqnslk"
   },
   "outputs": [],
   "source": [
    "from pyspark import RDD\n",
    "from typing import Tuple, List, TypeVar\n",
    "\n",
    "T = TypeVar('T')\n",
    "U = TypeVar('U')\n",
    "V = TypeVar('V')\n",
    "\n",
    "# calculates all triples (x, y, z) such that (x,y)∈r and (y,z)∈s\n",
    "def join(r: RDD[Tuple[T, U]], s: List) -> RDD[Tuple[T, U, V]]:\n",
    "  s = sc.broadcast(s)\n",
    "  join = r.flatMap(lambda x: ((x[0], x[1], _s[1]) for _s in s.value if _s[0] == x[1]))\n",
    "  return join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1DwxtRn7cce",
    "outputId": "d3062c4c-8493-4035-a451-b959e627be13"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 4), (1, 3, 4), (1, 3, 5), (2, 2, 4)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join(A, B).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksT9dLxgl9je"
   },
   "source": [
    "<center><img src='https://drive.google.com/uc?id=1BXZ0u3562N_MqCLcekI-Ens77Kk4LpPm'></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
