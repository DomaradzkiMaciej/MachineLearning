{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "rqmU96VmEukV",
    "outputId": "f28fbe49-ac8b-40d1-8c10-c103c1f82719",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "GPU device not found, selection Runtime -> Change runtime type",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mSystemError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_70843/1575813807.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcuda\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_available\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m     \u001B[0;32mraise\u001B[0m \u001B[0mSystemError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"GPU device not found, selection Runtime -> Change runtime type\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mSystemError\u001B[0m: GPU device not found, selection Runtime -> Change runtime type"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import Subset, TensorDataset, DataLoader\n",
    "from random import shuffle\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemError(\"GPU device not found, selection Runtime -> Change runtime type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nytMVhfFEukX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qICwccnqEukY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'lr': 1e-3, \n",
    "    'bs': 128, \n",
    "    'epochs': 5, \n",
    "    'num_tasks': 5,\n",
    "    'dataset': \"MNIST\",\n",
    "    'num_classes': 10, \n",
    "    'in_size': 28,\n",
    "    'n_channels': 1,\n",
    "    'hidden_size': 50\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J82g1SfMEukZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMUReodVEuka",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_dataset(dataroot, dataset):\n",
    "    if dataset == 'MNIST':\n",
    "        mean, std = (0.1307), (0.3081)\n",
    "    elif dataset == 'CIFAR10':\n",
    "        mean, std = (0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)\n",
    "\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=mean, std=std)])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.__dict__[dataset](\n",
    "        root=dataroot,\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    val_dataset = torchvision.datasets.__dict__[dataset](\n",
    "        root=dataroot,\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=transform\n",
    "    )\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "def split_dataset(dataset, tasks_split):\n",
    "    split_dataset = {}\n",
    "    for e, current_classes in tasks_split.items():\n",
    "        task_indices = np.isin(np.array(dataset.targets), current_classes)\n",
    "        split_dataset[e] = Subset(dataset, np.where(task_indices)[0])\n",
    "    return split_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cf4XCH7wEukb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Metrics & plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5i7obHPEukb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dict2array(acc):\n",
    "    num_tasks = len(acc)\n",
    "    first_task = list(acc.keys())[0]\n",
    "    sequence_length = len(acc[first_task]) if isinstance(acc[first_task], list) else num_tasks\n",
    "    acc_array = np.zeros((num_tasks, sequence_length))\n",
    "    for task, val in acc.items():\n",
    "        acc_array[int(task), :] = val\n",
    "    return acc_array\n",
    "\n",
    "\n",
    "def plot_accuracy_matrix(array):\n",
    "    num_tasks = array.shape[1]\n",
    "    array = np.round(array, 2)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(array, vmin=np.min(array), vmax=np.max(array))\n",
    "    for i in range(len(array)):\n",
    "        for j in range(array.shape[1]):\n",
    "            ax.text(j,i, array[i,j], va='center', ha='center', c='w', fontsize=15)\n",
    "    ax.set_yticks(np.arange(num_tasks))\n",
    "    ax.set_ylabel('Number of tasks')\n",
    "    ax.set_xticks(np.arange(num_tasks))\n",
    "    ax.set_xlabel('Tasks finished')\n",
    "    ax.set_title(f\"ACC: {np.mean(array[:, -1]):.3f} -- std {np.std(np.mean(array[:, -1])):.3f}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_acc_over_time(array):\n",
    "    fig, ax = plt.subplots()\n",
    "    for e, acc in enumerate(array):\n",
    "        ax.plot(acc, label=e)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def compute_average_accuracy(array):\n",
    "    num_tasks = len(array)\n",
    "    avg_acc = np.sum(array[:, -1], axis=0)/num_tasks\n",
    "    return avg_acc\n",
    "\n",
    "\n",
    "def compute_backward_transfer(array):\n",
    "    num_tasks = len(array)\n",
    "    diag = np.diag(array)[:-1] # Note, we do not compute backward transfer for the last task!\n",
    "    end_acc = array[:-1, -1]\n",
    "    bwt = np.sum(end_acc - diag)/(num_tasks - 1)\n",
    "    return bwt\n",
    "\n",
    "\n",
    "def compute_forward_transfer(array, b):\n",
    "    num_tasks = len(array)\n",
    "    sub_diag = np.diag(array, k=-1) # Note, we do not compute forward transfer for the first task!\n",
    "    fwt = np.sum(sub_diag - b[1:])/(num_tasks - 1)\n",
    "    return fwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1gHTG5m9Eukd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GreedyBuffer:\n",
    "    def __init__(self, samples_per_class):\n",
    "        self.samples_per_class = samples_per_class\n",
    "        self.samples = torch.Tensor([])\n",
    "        self.targets = torch.Tensor([])\n",
    "\n",
    "    def store_data(self, loader):\n",
    "        samples, targets = torch.Tensor([]), torch.Tensor([])\n",
    "        for sample, target in loader:\n",
    "            samples = torch.cat((samples, sample))\n",
    "            targets = torch.cat((targets, target))\n",
    "        \n",
    "        for label in torch.unique(targets):\n",
    "            greedy_idx = torch.where(targets == label)[0][:self.samples_per_class]\n",
    "            self.samples = torch.cat((self.samples, samples[greedy_idx]))\n",
    "            self.targets = torch.cat((self.targets, targets[greedy_idx]))\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.samples, self.targets.to(torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.samples) == len(self.targets), f\"Incosistent lengths of data tensor: {self.samples.shape}, target tensor: {self.targets.shape}!\"\n",
    "        return len(self.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torchgan.models import ConditionalGANGenerator, ConditionalGANDiscriminator\n",
    "from torchgan.losses import MinimaxGeneratorLoss, MinimaxDiscriminatorLoss\n",
    "from torchgan.trainer import Trainer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dcgan = Trainer(\n",
    "            {\"generator\": {\"name\": ConditionalGANGenerator, \n",
    "                           \"args\": {\"num_classes\": 10, \"out_channels\": 3, \"step_channels\": 16}, \n",
    "                           \"optimizer\": {\"name\": torch.optim.Adam, \"args\": {\"lr\": 0.0002, \"betas\": (0.5, 0.999)}}\n",
    "                          },\n",
    "             \"discriminator\": {\"name\": ConditionalGANDiscriminator, \n",
    "                               \"args\": {\"num_classes\": 10, \"in_channels\": 3, \"step_channels\": 16}, \n",
    "                               \"optimizer\": {\"name\": torch.optim.Adam, \"args\": {\"lr\": 0.0002, \"betas\": (0.5, 0.999)}}\n",
    "                              }\n",
    "            },\n",
    "            [MinimaxGeneratorLoss(), MinimaxDiscriminatorLoss()],\n",
    "            sample_size=64, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train, test = get_dataset(dataroot='../data/', dataset=args['dataset'])\n",
    "train_tasks = split_dataset(train, class_split)\n",
    "val_tasks = split_dataset(test, class_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xU05EXpnEukd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Class incremental model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cc7QfgI0Euke",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Agent:\n",
    "    def __init__(self, args, train_datasets, val_datasets):\n",
    "        self.args = args\n",
    "        self.model = MLP(self.args)\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.cuda()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args['lr'])\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        self.reset_acc()\n",
    "        self.train_datasets = train_datasets\n",
    "        self.val_datasets = val_datasets\n",
    "        self.buffer = GreedyBuffer(samples_per_class = 100)\n",
    "    \n",
    "    def reset_acc(self):\n",
    "        self.acc = {key: [] for key in self.args['task_names']}\n",
    "        self.acc_end = {key: [] for key in self.args['task_names']}\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for task, data in self.train_datasets.items():\n",
    "            self.model = MLP(self.args)\n",
    "            if torch.cuda.is_available():\n",
    "                self.model.cuda()\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.args['lr'])\n",
    "            loader = DataLoader(data, batch_size=self.args['bs'])\n",
    "            self.buffer.store_data(loader)\n",
    "            print(f\"Task {task} -- buffer stores {len(self.buffer)} samples\")\n",
    "            samples, targets = self.buffer.get_data()\n",
    "            greedy_dataset = TensorDataset(samples, targets)\n",
    "            greedy_loader = DataLoader(greedy_dataset, batch_size=self.args['bs'], shuffle=True)\n",
    "            for epoch in range(self.args['epochs']):\n",
    "                epoch_loss = 0\n",
    "                total = 0\n",
    "                correct = 0\n",
    "                for e, (X, y) in enumerate(greedy_loader):\n",
    "                    if torch.cuda.is_available():\n",
    "                        X, y = X.cuda(), y.cuda()\n",
    "                    output = self.model(X)\n",
    "                    loss = self.criterion(output, y)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward() \n",
    "                    self.optimizer.step()\n",
    "                    epoch_loss += loss.item()\n",
    "                    correct += torch.sum(torch.topk(output, axis=1, k=1)[1].squeeze(1) == y)\n",
    "                    total += len(X)\n",
    "                    if e % 50 == 0:\n",
    "                        self.validate()\n",
    "                print(f\"Epoch {epoch}: Loss {epoch_loss/(e+1):.3f} Acc: {correct/total:.3f}\")\n",
    "            self.validate(end_of_epoch=True)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, end_of_epoch=False):\n",
    "        self.model.eval()\n",
    "        for task, data in self.val_datasets.items():\n",
    "            loader = torch.utils.data.DataLoader(data, batch_size=args['bs'], shuffle=True)\n",
    "            correct, total = 0, 0\n",
    "            for e, (X, y) in enumerate(loader):\n",
    "                if torch.cuda.is_available():\n",
    "                    X, y = X.cuda(), y.cuda()\n",
    "                output = self.model(X)\n",
    "                correct += torch.sum(torch.topk(output, axis=1, k=1)[1].squeeze(1) == y).item()\n",
    "                total += len(X)\n",
    "            self.acc[task].append(correct/total)\n",
    "            if end_of_epoch:\n",
    "                self.acc_end[task].append(correct/total)\n",
    "        self.model.train()\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        hidden_size = args['hidden_size']\n",
    "        self.fc1 = torch.nn.Linear(args['in_size']**2 * args['n_channels'], hidden_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = torch.nn.Linear(hidden_size, args['num_classes'])\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = input.flatten(start_dim=1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = torch.nn.functional.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ct2ZJbUEukf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classes = list(range(args['num_classes']))\n",
    "shuffle(classes)\n",
    "class_split = {str(i): classes[i*2: (i+1)*2] for i in range(args['num_tasks'])}\n",
    "args['task_names'] = list(class_split.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-YzKJXOEukg",
    "outputId": "84b4f315-7ed0-4ce8-f7d8-21e5b0360cd5",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train, test = get_dataset(dataroot='../data/', dataset=args['dataset'])\n",
    "train_tasks = split_dataset(train, class_split)\n",
    "val_tasks = split_dataset(test, class_split)\n",
    "agent = Agent(args, train_tasks, val_tasks)\n",
    "\n",
    "agent.validate()\n",
    "random_model_acc = [i[0] for i in agent.acc.values()]\n",
    "agent.reset_acc()\n",
    "agent.train()\n",
    "\n",
    "acc_at_end_arr = dict2array(agent.acc_end)\n",
    "plot_accuracy_matrix(acc_at_end_arr)\n",
    "\n",
    "acc_arr = dict2array(agent.acc)\n",
    "plot_acc_over_time(acc_arr)\n",
    "\n",
    "print(f\"The average accuracy at the end of sequence is: {compute_average_accuracy(acc_at_end_arr):.3f}\")\n",
    "print(f\"BWT:'{compute_backward_transfer(acc_at_end_arr):.3f}'\")\n",
    "print(f\"FWT:'{compute_forward_transfer(acc_at_end_arr, random_model_acc):.3f}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "main_with_solutions.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}